## Objectives

At the end of this lecture, you will be able to

- implement the K-means and the alternative K-medoids (or partitioning around medoids) algorithms.

- Inspect an elbow plot to choose the number of clusters for K-means.

- Know the principals behind the expectation-maximization (EM) algorithm as an algorithm to estimate the maximum likelihood estimates for the parameters of a Gaussian Mixture Model (GMM) .

- Perform agglomerative clustering using different dissimilarity measures and read the associated dendrogram .

- Know the principals behind the density-based spatial clustering of applications with noise (DBSCAN) .

- Evaluate cluster assignments using silouette scores and plots .

##  Motivation

Clustering aims to group of data points together so that within the same group, the observations are very similar, and between different groups, the observations are dissimilar, according to some dissimilarity measure.

The data points do not need to come with any labels. Often, clustering is used as a way to decide what and how many class labels are suitable for the data set. For example, we may want to cluster gene expression data into groups that will characterize different cell types in a blood sample. You will perform such analysis as the homework at the end of this modules.

In this lecture, we will discuss in more detail the common clustering methods motivated in the video above, from algorithms for which we need to first decide the number of clusters, such as K-means and the Gaussian mixture models, to more general approaches such as hierarchical clustering and DBSCAN.

## K-Means

**ğ¾-Means Loss Function**

K-means starts with a pre-selected number of clusters  ğ¾ , and is a clustering algorithm that aims to minimize the within group sums of squares (WGSS) :

 	 WGSS 	 = 	 âˆ‘ğ‘˜=1ğ¾âˆ‘ğ±(ğ‘–),ğ±(ğ‘—)âˆˆğ¶ğ‘˜ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—))2 	 	 
where the  ğ‘˜  indexes the  ğ¾  different clusters,  ğ¶ğ‘˜  denotes the  ğ‘˜ -th cluster, and  ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—))  is the distance between the data points  ğ±(ğ‘–)  and  ğ±(ğ‘—) . The distance is commonly the Euclidean distance.

The  WGSS  measures how dissimilar data points in the same cluster are. To find an algorithm to minimize  WGSS , it helps to first rewrite  WGSS  in terms of the means  ğğ‘˜=âˆ‘ğ±(ğ‘–)âˆˆğ¶ğ‘˜ğ±(ğ‘–)/ğ‘›ğ‘˜  of each cluster  ğ¶ğ‘˜ :

 	 WGSS 	 = 	 âˆ‘ğ‘˜=1ğ¾âˆ‘ğ±(ğ‘–),ğ±(ğ‘—)âˆˆğ¶ğ‘˜â€–â€–ğ±(ğ‘–)âˆ’ğ±(ğ‘—)â€–â€–2 	 	 
 	 	 = 	 âˆ‘ğ‘˜=1ğ¾âˆ‘ğ±(ğ‘–),ğ±(ğ‘—)âˆˆğ¶ğ‘˜â€–â€–(ğ±(ğ‘–)âˆ’ğğ‘˜)âˆ’(ğ±(ğ‘—)âˆ’ğğ‘˜)â€–â€–2 	 	 
 	 	 = 	 âˆ‘ğ‘˜=1ğ¾âˆ‘ğ±(ğ‘–),ğ±(ğ‘—)âˆˆğ¶ğ‘˜(â€–â€–(ğ±(ğ‘–)âˆ’ğğ‘˜)â€–â€–2+â€–â€–ğ±(ğ‘—)âˆ’ğğ‘˜)â€–â€–2âˆ’2(ğ±(ğ‘–)âˆ’ğğ‘˜)â‹…(ğ±(ğ‘—)âˆ’ğğ‘˜)) 	 	 
 	 	 = 	 âˆ‘ğ‘˜=1ğ¾â›ââœâœğ‘›ğ‘˜âˆ‘ğ±(ğ‘–)âˆˆğ¶ğ‘˜â€–â€–(ğ±(ğ‘–)âˆ’ğğ‘˜)â€–â€–2+ğ‘›ğ‘˜âˆ‘ğ±(ğ‘—)âˆˆğ¶ğ‘˜â€–â€–ğ±(ğ‘—)âˆ’ğğ‘˜)â€–â€–2âˆ’2â›ââœâœâˆ‘ğ±(ğ‘–)âˆˆğ¶ğ‘˜(ğ±(ğ‘–)âˆ’ğğ‘˜)ââ âŸâŸâ‹…â›ââœâœâˆ‘ğ±(ğ‘—)âˆˆğ¶ğ‘˜(ğ±(ğ‘—)âˆ’ğğ‘˜)ââ âŸâŸââ âŸâŸ 	 	 
 	 	 = 	 2ğ‘›ğ‘˜âˆ‘ğ‘˜=1ğ¾âˆ‘ğ±(ğ‘–)âˆˆğ¶ğ‘˜â€–â€–(ğ±(ğ‘–)âˆ’ğğ‘˜)â€–â€–2since âˆ‘ğ±(ğ‘–)âˆˆğ¶ğ‘˜(ğ±(ğ‘–)âˆ’ğğ‘˜)=0. 	 	 

**ğ¾-Means Algorithm**

ğ¾ -means minimizes the  WGSS  by the following iterative algorithm.

First, initialize the  ğ¾  means  {ğğ‘˜}ğ‘˜=1,â€¦ğ¾  to random positions.

Then, repeat the two steps below until the algorithm converges:

Cluster assignment: Cluster each point with the closest centroid  ğğ‘˜ . Call the set of all points in this cluster  ğ¶ğ‘˜ .

Centroids Update. Update all centroids  ğğ‘˜  to be the average position of all points assigned to  ğ¶ğ‘˜  in the step above .

ğ¾ -means does not guarantee convergence to the global minimum, and in fact often converges to a local minimum depending on the random initialization of the cluster centroids. To approach the global minimum, conduct multiple runs starting with different randomly selected cluster centroids. This strategy tends to be more effective when the number of clusters is small (e.g.,  <10 ).

**ğ¾-medoids**

There are two limitations associated with K-means:

Its results are sensitive to outliers.

The cluster centroids are not necessarily data points.

To alleviate these two issues, we can modify the algorithm to use medoids instead of means. The medoid of a cluster is the data point that is closest to the mean of that cluster.

**Choosing K**

One heuristic method to determine the number of clusters  ğ¾  is the â€œelbow" method. Plot the loss function  WGSS  across different  ğ¾  values, and pick the  ğ¾  corresponding to the â€œelbow" of the plot. For example, according to the plot below, the best choice for  ğ¾  is 3.

## Gaussian Mixture Models (GMM) and Expectation-Maximization (EM) Algorithm

Clustering using Gaussian mixture model (GMM) generalizes  ğ¾ -mean in two ways:

- Cluster assignment is based on the probabilities of the data point being generated by the different clusters.

- The shape of the clusters can be elliptical rather than only spherical.

**Estimating the Parameters of Gaussian Mixture Models (GMM) using the Expectation-Maximization (EM) Algorithm**

Consider the Gaussian mixture model of  ğ¾  Gaussians:

 	 ğ(ğ—)=âˆ‘ğ‘˜=1ğ¾ğ‘ğ‘˜ğ(ğ—|cluster ğ‘˜) 	 	 
where

 	 ğ‘ğ‘˜ 	 = 	 ğ(cluster ğ‘˜) 	 	 
 	 ğ—|cluster ğ‘˜ 	 âˆ¼ 	 îˆº(ğğ‘˜,Î£ğ‘˜). 	 	 
Here,  ğ(ğ—)  denotes the probability of obtaining the observation  ğ— , and  ğ(ğ—|cluster ğ‘˜)  is the probability of obtaining the observation  ğ—  given that it is generated by the model for cluster  ğ‘˜ . This mixture has parameters  ğœƒ={ğ‘1,â€¦ğ‘ğ‘˜,ğ1,â€¦,ğğ¾,Î£1,â€¦,Î£ğ¾} . They correspond to the mixing proportions, means, and covariance matrices of each of the  ğ¾  Gaussians.

Given  ğ‘›  data points  ğ±(1),â€¦,ğ±(ğ‘›)  in  â„ğ‘‘ , our goal is to set our parameters  ğœƒ  to maximize the data log-likelihood:

 	 â„“(ğ±(1),â€¦,ğ±(ğ‘›);ğœƒ) 	 = 	 logâˆğ‘–=1ğ‘›ğ(ğ±(ğ‘–);ğœƒ)=âˆ‘ğ‘–=1ğ‘›log[âˆ‘ğ‘˜=1ğ¾ğ‘ğ‘˜ğ(ğ±(ğ‘–)|cluster ğ‘˜;ğœƒ)]. 	 	 
There is no closed-form solution to finding the parameter set  ğœƒ  that maximizes this likelihood. The EM algorithm is an iterative algorithm that finds a locally optimal solution  ğœƒÌ‚   to the GMM likelihood maximization problem.

**E Step**

The E Step of the algorithm involves finding the posterior probability  ğ‘(ğ‘˜âˆ£ğ‘–)=ğ(cluster ğ‘˜|ğ±(ğ‘–);ğœƒ)  that point  ğ±(ğ‘–)  was generated by cluster  ğ‘˜ , for every  ğ‘–=1,â€¦,ğ‘›  and  ğ‘˜=1,â€¦,ğ¾ . This step assumes the knowledge of the parameter set  ğœƒ . We find the posterior using Bayes' rule:

 	 ğ‘(ğ‘˜âˆ£ğ‘–)=ğ(cluster ğ‘˜|ğ±(ğ‘–);ğœƒ) 	 = 	 ğ‘ğ‘˜ğ(ğ±(ğ‘–)|cluster ğ‘˜;ğœƒ)ğ(ğ±(ğ‘–);ğœƒ)=ğ‘ğ‘˜îˆº(ğ±(ğ‘–);ğœ‡(ğ‘˜),Î£ğ‘˜)âˆ‘ğ¾ğ‘—=1ğ‘ğ‘˜îˆº(ğ±(ğ‘–);ğœ‡(ğ‘—),Î£ğ‘—) 	 	 
**M Step**

The M Step of the algorithm maximizes the expected log likelihood function  â„“Ìƒ (ğ±(1),â€¦,ğ±(ğ‘›)ğœƒ) , which is a lower bound on the log-likelihood. The algorithm thus iteratively pushes the data likelihood upwards.

The expected log likelihood function  â„“Ìƒ (ğ±(1),â€¦,ğ±(ğ‘›)ğœƒ)  is

 	 â„“Ìƒ (ğ±(1),â€¦,ğ±(ğ‘›);ğœƒ) 	 = 	 âˆ‘ğ‘–=1ğ‘›[âˆ‘ğ‘˜=1ğ¾ğ‘(ğ‘˜âˆ£ğ‘–)log(ğ(ğ±(ğ‘–),cluster ğ‘˜;ğœƒ)ğ‘(ğ‘˜âˆ£ğ‘–))] 	 	 
 	 	 = 	 âˆ‘ğ‘–=1ğ‘›[âˆ‘ğ‘˜=1ğ¾ğ‘(ğ‘˜âˆ£ğ‘–)log(ğ‘ğ‘˜îˆº(ğ±(ğ‘–);ğœ‡(ğ‘˜),Î£ğ‘˜)ğ‘(ğ‘˜âˆ£ğ‘–))] 	 	 
where recall the short hand notation for the posterior distribution is  ğ‘(ğ‘˜âˆ£ğ‘–)=ğ(cluster ğ‘˜|ğ±(ğ‘–);ğœƒ) .

This expected log likelihood function is a lower bound on the actual log-likelihood

 	 â„“(ğ±(1),â€¦,ğ±(ğ‘›);ğœƒ)=âˆ‘ğ‘–=1ğ‘›log[âˆ‘ğ‘˜=1ğ¾ğ(ğ±(ğ‘–),cluster ğ‘˜;ğœƒ)]. 	 	 
(This is due to Jensen's inequality.)

In the special case where the covariance matrix pf the  ğ‘˜ -th Gaussian is  Î£ğ‘˜=ğœ2ğ‘˜ğˆ , the parameters that maximize the above expected log likelihood function are as follows.

 	 ğ(ğ‘˜)Ë† 	 =âˆ‘ğ‘›ğ‘–=1ğ±(ğ‘–)ğ‘(ğ‘˜âˆ£ğ‘–)âˆ‘ğ‘›ğ‘–=1ğ‘(ğ‘˜âˆ£ğ‘–) 	 	 
 	 ğ‘ğ‘˜Ë† 	 =1ğ‘›âˆ‘ğ‘–=1ğ‘›ğ‘(ğ‘˜âˆ£ğ‘–), 	 	 
 	 ğœ2ğ‘˜Ë† 	 =âˆ‘ğ‘›ğ‘–=1ğ‘(ğ‘˜âˆ£ğ‘–)â€–ğ±(ğ‘–)âˆ’ğœ‡(ğ‘˜)Ë†â€–2ğ‘‘âˆ‘ğ‘›ğ‘–=1ğ‘(ğ‘˜âˆ£ğ‘–) 	 	 
You can verify this by taking derivatives and setting them equal to zero.

The E and M steps are repeated iteratively until there is no noticeable change in the actual likelihood computed after M step using the newly estimated parameters or if the parameters do not vary by much.

**Initialization**

for the initialization before the first time E step is carried out, we can either do a random initialization of the parameter set  ğœƒ  or we can employ K-means to find the initial cluster centers of the  ğ¾  clusters and use the global variance of the dataset as the initial variance of all the  ğ¾  clusters. In the latter case, the mixture weights can be initialized to the proportion of data points in the clusters as found by the k-means algorithm.


## Hierarchical Clustering

Hierarchical clustering does not start with a fixed chosen number of clusters, but builds a hierarchy of clusters with different levels corresponding to different numbers of clusters.

We can use a bottom-up or top-down approach:

- Agglomerative clustering (Bottom-Up)

- Divisive clustering (Top-down)

We will only discuss the bottom-up approach.

**Agglomerative Clustering (Bottom-Up approach)**

**Agglomerative clustering** starts with 1 data point per cluster, and at each consequent stage, merges pairs of clusters that are the closest together according to a dissimilarity measure between clusters.

This merging can be depicted by a tree, also known as a dendrogram . The bottom-most level has  ğ‘›  clusters (of 1 observation each), and as merging occurs as the levels go up, the number of clusters decreases, and the top-most level has only  1  cluster (encompassing all observations). See the recitation in this module for an example of hierarchical clustering and the associated dendrogram.

**Dissimilarity between clusters**

In order to choose which pair of clusters to merge at each stage, we need to define a dissimilarity measure between clusters, and the dissimilarity measure between clusters is often based on dissimilarity between points. A few commonly used distances between individual points are:

- ğ‘™2âˆ’ norm, i.e. the usual Euclidean distance

 	 ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—))=(ğ‘¥(ğ‘–)1âˆ’ğ‘¥(ğ‘—)1)2+(ğ‘¥(ğ‘–)2âˆ’ğ‘¥(ğ‘—)2)2+â‹¯+(ğ‘¥(ğ‘–)ğ‘âˆ’ğ‘¥(ğ‘—)ğ‘)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš(ğ±(ğ‘–)âˆˆâ„ğ‘) 	 	 
- ğ‘™1âˆ’ norm (also known as Manhattan distance)

 	 ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—))=âˆ£âˆ£ğ‘¥(ğ‘–)1âˆ’ğ‘¥(ğ‘—)1âˆ£âˆ£+âˆ£âˆ£ğ‘¥(ğ‘–)2âˆ’ğ‘¥(ğ‘—)2âˆ£âˆ£+â‹¯+âˆ£âˆ£ğ‘¥(ğ‘–)ğ‘âˆ’ğ‘¥(ğ‘—)ğ‘âˆ£âˆ£(ğ±(ğ‘–)âˆˆâ„ğ‘) 	 	 
- ğœ†âˆâˆ’ norm, i.e. the maximum distance among all coordinates

 	 ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—))=maxğ‘˜=1,â€¦,ğ‘âˆ£âˆ£ğ‘¥(ğ‘–)ğ‘˜âˆ’ğ‘¥(ğ‘—)ğ‘˜âˆ£âˆ£(ğ±(ğ‘–)âˆˆâ„ğ‘) 	 	 
- Other dissimilarity measures  ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—))  that do not satisfy all properties of distances, but still meet the following criteria:

 	 ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—)) 	 â‰¥ 	 0(positivity) 	 	 
 	 ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘–)) 	 = 	 0 	 	 
 	 ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—)) 	 = 	 ğ‘‘(ğ±(ğ‘—),ğ±(ğ‘–))(symmetry) 	 	 
Now we can define dissimalarity measures between clusters:

- **Minimum distance** between points in the two clusters, also known as single linkage :

 	 ğ‘‘(ğ¶1,ğ¶2) 	 = 	 minğ±(ğ‘–)âˆˆğ¶1,ğ±(ğ‘—)âˆˆğ¶2ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—)). 	 	 
- **Maximum distance** between points in the two clusters, also known as complete linkage :

 	 ğ‘‘(ğ¶1,ğ¶2) 	 = 	 maxğ±(ğ‘–)âˆˆğ¶1,ğ±(ğ‘—)âˆˆğ¶2ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—)). 	 	 
- **Average distance** between points in the two different clusters, also known as average linkage

 	 ğ‘‘(ğ¶1,ğ¶2) 	 = 	 1ğ‘›1ğ‘›2âˆ‘ğ±(ğ‘–)âˆˆğ¶1âˆ‘ğ±(ğ‘—)âˆˆğ¶2ğ‘‘(ğ±(ğ‘–),ğ±(ğ‘—)). 	 



## DBSCAN

Density-based spatial clustering of applications with noise (DBSCAN) aims to cluster together points that are close to each another in a dense region, and leave out points that are in low density regions.

To perform DBSCAN, we need to choose two parameters:

- ğœ– , distance between connected points.

- ğ‘˜ , core strength

Let two points be connected if they are within a distance  ğœ–  of one another. A core point is a point that are connected to at least  ğ‘˜  other points.

Then two points are placed into the same cluster if and only if there is a connecting path between them consisting of only core points, except possibly at the ends of the path.


In the figure above, the blue points are core points for core strength  ğ‘˜=4 , since they are each connected to at least 4 other points. Two clusters are formed. In each cluster, each non-core (black) point is connected to a core point (blue) . The points that are not connected to any core points are outliers and not clustered into any cluster in DBSCAN.
