## Objectives

At the end of this lecture, you should be able to:

- Recognize the different features of the three dimension reduction techniques: principal component analysis (PCA), multi-dimensional scaling (MDS), and stochastic neighbor embedding (SNE) .

- Implement the PCA and the classical- MDS algorithms using elementary linear algebra.

- Decide between using covariance and correlation for the dimension reduction analysis.

- Explore how to use explained variance to choose the number of dimensions of the projection.

- Describe Stochastic Neighbor Embedding (SNE) and t-distributed Stochastic Neighbor Embedding (t-SNE) as minimization of the Kullback-Leibler divergence (KL-divergence) between the probability distributions on data pairs in both the original high-dimensional and the target low-dimensional space.

- Recognize that t-SNE alleviates crowding of data points in low-dimension.

- Analyze the results of PCA, MDS, t-SNE together to identify patterns of the data.

## Introduction
[Wikipedia - Singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)
[Wikipedia - MDS](https://en.wikipedia.org/wiki/Multidimensional_scaling)


Dimension reduction refers to a set of techniques which can transform high-dimensional data into their representative low-dimensional data. During the process, some information of the original data is discarded but some main characteristics of the original data is preserved.

Dimension reduction is important because processing and analyzing high-dimensional data can be computationally intractable. Dimension reduction is very useful in dealing with a large number of observations and variables, hence it is widely used in many fields such as signal processing, machine learning, and bioinformatics.

Three dimension reduction techniques will be introduced:

- Principal Component Analysis (PCA)

- Multidimensional Scaling (MDS), and

- Stochastic Neighbor Embedding (SNE).

**PCA** tries to project the original high-dimensional data into lower dimensions by capturing the most prominent variance in the data.

**MDS** is a technique for reducing data dimensions while attempting to preserve the relative distance between high-dimensional data points.

**SNE** is a non-linear technique to â€œcluster" data points by trying to keep similar data points close to each other.

PCA and classical MDS share similar computations: they both use the spectral decomposition of symmetric, but on different input matrices.

You will think through some of the mathematics in this lecture. For demonstration of implementations, see the recitation at the end of this module.

## Principal Component Analysis (PCA)


**Principal component analysis (PCA)** is often used to find a low dimensional representation of data that maximizes the spread of the projected data.

The **first principal component ( PC1 )** is the direction of the largest variance of the data. The **second principal component  PC2 )** is perpendicular to the first principal component and is the direction of the largest variance of the data among all directions that are perpendicular to the first principal component. The **third principal component ( PC3 )** is perpendicular to both first and second principal components and is in the direction of the largest variance among all directions that are perpendicular to both the first and second principal components. This can continue until we obtain as many principal components as the dimension of the original space in which the data is given, i.e. an orthogonal basis of the data space consisting of principal components. However, we are generally interested in the fewer dimensions as the original space, for example, 2 or 3 dimensions for visualization, or few than 100 dimensions out of thousands of dimensions of the original space, as you will see in the recitation and the analysis at the end of this module.

Principal component analysis can be formulated in the following three equivalent ways. For simpliciy, we will only formulate the problem for the first principal component  PC1 .

Let  ğ±(1),ğ±(2),ğ±(ğ‘›)âˆˆâ„ğ‘  denote the  ğ‘›  data points in  ğ‘  dimensional space. The first principal component is the line spanned by a unit vector  ğ°âˆˆâ„ğ‘  such that

- ğ°  minimizes the sum of squared residuals of the orthogonal projections of data  ğ±(ğ‘–)  onto  ğ° :

 	 minğ°âˆˆâ„ğ‘âˆ‘ğ‘–=1ğ‘›â€–â€–ğ±(ğ‘–)âˆ’(ğ±(ğ‘–)â‹…ğ°)ğ°â€–â€–2 	 	 
- ğ°  maximizes the sum of squared norms of the orthogonal projections of data  ğ±(ğ‘–)  onto  ğ° :

 	 maxğ°âˆˆâ„ğ‘âˆ‘ğ‘–=1ğ‘›â€–â€–(ğ±(ğ‘–)â‹…ğ°)â€–â€–2 	 

- ğ°  is an eigenvector corresponding to the largest eigenvalue of the the sample covariance matrix  ğ’ :

 	 ğ’ 	 = 	 1ğ‘›âˆ’1ğ•ğ‘‡ğ•where ğ•=â›ââœâœâœâœâœâ†â†â†(ğ±(1))ğ‘‡(ğ±(2))ğ‘‡â‹®(ğ±(ğ‘›))ğ‘‡â†’â†’â†’ââ âŸâŸâŸâŸâŸ. 	 	 

**Remark:** Note that the sample covariance matrix has different definitions (dividing by  ğ‘›  or  ğ‘›âˆ’1 , but the eigenvectors and the order of the eigenvalues are not affected by this overall rescaling. Hence, if our goal is only to find the principal components, we are free to choose any scalar multiple of sample covariance matrix, e.g.  ğ•ğ‘‡ğ• .

## Correlation matrix
Recall that the correlation  ğœŒğ‘‹,ğ‘Œ  of two random variables  ğ‘‹  and  ğ‘Œ  is the covariance of the the rescaled random variables  ğ‘‹ğœğ‘‹ , and  ğ‘Œğœğ‘Œ :

 	 ğœŒğ‘‹,ğ‘Œ 	 = 	 ğ–¢ğ—ˆğ—(ğ‘‹ğœğ‘‹,ğ‘Œğœğ‘Œ)=ğ–¢ğ—ˆğ—(ğ‘‹,ğ‘Œ)ğœğ‘‹ğœğ‘Œ=ğ„[ğ‘‹ğ‘Œ]âˆ’ğ„[ğ‘‹]ğ„[ğ‘Œ](ğ„[ğ‘‹2]âˆ’(ğ„[ğ‘‹])2)(ğ„[ğ‘Œ2]âˆ’(ğ„[ğ‘Œ])2)â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš. 	 	 
If  ğ„[ğ‘‹]=ğ„[ğ‘Œ]=0 , then

 	 ğœŒğ‘‹,ğ‘Œ 	 = 	 ğ„[ğ‘‹ğ‘Œ]ğ„[ğ‘‹2]ğ„[ğ‘Œ2]â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš. 	 	 
Extending to random vectors  ğ—  with  ğ„[ğ—]=0 , the correlation matrix is the covariance matrix of rescaled data  ğ—Ìƒ ğ‘‡=(ğ‘‹1/ğœ1ğ‘‹2/ğœ2â‹¯ğ‘‹ğ‘›/ğœğ‘›)ğ‘‡ :

 	 ğ„[ğ—Ìƒ ğ—Ìƒ ğ‘‡]=ğ„â¡â£â¢â¢â¢â¢â›ââœâœâœâœğ‘‹1/ğœ1ğ‘‹2/ğœ2â‹®ğ‘‹ğ‘›/ğœğ‘›ââ âŸâŸâŸâŸ(ğ‘‹1/ğœ1ğ‘‹2/ğœ2â‹¯ğ‘‹ğ‘›/ğœğ‘›)â¤â¦â¥â¥â¥â¥ 	 	 
so that  ğ‘–,ğ‘— -th entry of the correlation matrix is  ğ„[ğ—ğ‘–ğœğ‘–ğ—ğ‘—ğœğ‘—] .

By definition, covariance has the dimension of squared of the data  ğ‘¥2 , while correlation is dimensionless. Hence, when covariates have different dimensions or units, it makes more sense to first rescale each covariate by its sample standard deviation, before determining the directions of largest spread using PCA.

## Total variance
We can decide how many principal components to retain in the projection of the data based on their explained variance.

The total variance is the sum of variance of all the principal components, which equals the sum of the eigenvalues of the covariance matrix:

Total Variance:âˆ‘ğ‘—=1ğ‘ğœ†ğ‘—. 
 
The fraction of variance explained by a principal component is the ratio of the variance of that principal component to the total variance:

 	 ğœ†ğ‘–âˆ‘ğ‘ğ‘—=1ğœ†ğ‘— 	 	 
Ungraded Exercise: Compare the explained variance of the first principal component when PCA is applied to the correlation matrix with when PCA is applied to the covariance matrix.

## MDS
The goal of multidimensional scaling (MDS) is to preserve the relative distance between high-dimensional vectors in a low-dimensional space (usually 2 or 3).

**Input of MDS**

This technique can operate without the original dataset  ğ•  ( ğ•âˆˆâ„ğ‘›Ã—ğ‘ ). Instead, it can directly use the distance matrix  ğƒâˆˆâ„ğ‘›Ã—ğ‘› , where  ğƒğ‘–ğ‘—  is the distance between  ğ±(ğ‘–)  and  ğ±(ğ‘—) .

**Output**

The objective is to find points  ğ²1,â€¦,ğ²ğ‘›âˆˆâ„ğ‘  such that an objective, for example

âˆ‘ğ‘–=1ğ‘›âˆ‘ğ‘—=1ğ‘›(ğƒğ‘–ğ‘—âˆ’â€–ğ²ğ‘–âˆ’ğ²ğ‘—â€–2)2 
 
is minimized, where  ğ²ğ‘–  and  ğ²ğ‘—  are the projections of  ğ±ğ‘–  and  ğ±ğ‘—  in a low-dimensional space, respectively.

This objective requires the distance between two data points in the high-dimensional space to be similar to their distance in the low-dimensional space.

**Variation of objectives**

There are variations of MDS which use different objective functions. For example, weighted MDS uses the objective function  âˆ‘ğ‘›ğ‘–=1âˆ‘ğ‘›ğ‘—=1ğ‘¤ğ‘–ğ‘—(ğƒğ‘–ğ‘—âˆ’â€–ğ²ğ‘–âˆ’ğ²ğ‘—â€–2)2 , where  ğ‘¤ğ‘–ğ‘—  are non-negative weights.

*X is matrix consisting of row vectors x1, x2 ... xn*
*X @ X^T: the diagnol represent length of vector. [i,j] element represent angel (dot product)*


*Classical MDS is PCS on B = X@X^T; classical PCA operates on X^T@X*
*If sample size = n, feature size = p*
*Classical MDS operates on nxn matrix, while classical PCA operates on pxp matrix*

## Stochastic neighbor embedding (SNE)

Stochastic neighbor embedding (SNE) is a probabilistic approach to dimensional reduction that places data points in high dimensional space into low dimensional space while preserving the identity of neighbors. That is, SNE attempts to keep nearby data points nearby, and separated data points relatively far apart.

The idea of SNE is to define a probability distribution on pairs of data points in each of the original high dimensional space and the target low dimensional space, and then determine the placement of objects in low dimension by minimizing the â€œdifference' of the two probability distributions.

More precisely:

1. Input: Distance matrix  ğ·ğ‘–ğ‘—  of data in a  ğ‘ -dimensional space.

2. In high dimension: In the  ğ‘  dimensional space, center a Gaussian distribution ( exp(âˆ’â€–â€–ğ±âˆ’ğ±(ğ‘–)â€–â€–2)  on each data point  ğ±(ğ‘–) . For each data point  ğ±(ğ‘–) , define the probability of another data point  ğ±(ğ‘—)  being its neighbor to be

 	 ğ‘ğ‘–ğ‘—=exp(âˆ’ğ·2ğ‘–ğ‘—)âˆ‘ğ‘˜â‰ ğ‘™exp(âˆ’ğ·2ğ‘™ğ‘˜)where ğ·2ğ‘–ğ‘—=â€–â€–ğ±(ğ‘–)âˆ’ğ±(ğ‘—)â€–â€–2,ğ‘–â‰ ğ‘— 	 	 
where the denominator sums over all distinct pairs of data points. Notice the symmetry  ğ‘ğ‘–ğ‘—=ğ‘ğ‘—ğ‘– ; hence we can restrict to indices where  ğ‘–<ğ‘— .

The set of all  ğ‘ğ‘–ğ‘—  defines the pmf ofprobability distribution  ğ  on all pairs of points in  ğ‘ -dimensional space. The shape of the Gaussian distribution ensures that pairs that are close together are given much more weight than pairs that are far apart.

**Example 0:**

If a data set had only 2 data points  ğ±(1),ğ±(2)  at distance  ğ‘‘  in  ğ‘ -dimensional space, then using the definition above:

 	 ğ‘ğ‘–ğ‘—=exp(âˆ’ğ·2ğ‘–ğ‘—)âˆ‘ğ‘˜>ğ‘™exp(âˆ’ğ·2ğ‘™ğ‘˜)where ğ‘–<ğ‘—, 	 	 
we get

ğ‘12=exp(âˆ’ğ‘‘2)exp(âˆ’ğ‘‘2)=1 
 
regardless of the distance  ğ‘‘ , since there is only 1 pair of distinct data points.

3. In Low Dimensions: Do the same as above in  ğ‘ -dimensional target space. That is, define for each point  ğ²(ğ‘–)  the probability of  ğ²(ğ‘—)  being its neighbor to be

 	 ğ‘ğ‘–ğ‘—=exp(âˆ’â€–â€–ğ²(ğ‘–)âˆ’ğ²(ğ‘—)â€–â€–2)âˆ‘ğ‘˜â‰ ğ‘™exp(âˆ’â€–â€–ğ²(ğ‘˜)âˆ’ğ²(ğ‘™)â€–â€–2)(ğ‘–â‰ ğ‘—). 	 	 
The set of all  ğ‘ğ‘–ğ‘—  define the pmf of a probability distribution  ğ  on all pairs of points in the  ğ‘ -dimensional target space.

4. Minimization: Find points  {ğ²(ğ‘–)}  in the  ğ‘ -dimensional target space that minimizes the Kullback-Leibler (KL) divergence between the probability distributions  ğ  and  ğ :

KL(ğ||ğ)=âˆ‘ğ‘–â‰ ğ‘—ğ‘ğ‘–ğ‘—logğ‘ğ‘–ğ‘—ğ‘ğ‘–ğ‘— 
 
where  ğ‘ğ‘–ğ‘—  and  ğ‘ğ‘–ğ‘—  give the pmfs of  ğ  and  ğ  respectively.

In practice, this minimization is implemented using gradient descent methods.

Remark: For simplicity, in the definitions of the distributions  ğ  and  ğ , we have used Gaussian distributions with the same variance at each data point, and simplified the definition of  ğ‘ğ‘–ğ‘—  and  ğ‘ğ‘–ğ‘— . Practical algorithms are more sophisticated: they use Gaussian distributions with different variances  ğœ2ğ‘–  at different data points, and then symmetrize between points  ğ‘–  and  ğ‘—  to get the pmf  ğ‘ğ‘–ğ‘— . (See for example Wikipedia page on T-SNE.) The different variances  ğœ2ğ‘–  are chosen to match with user-defined hyperparameters such as  perplexity . You will have a chance to explore the effect of some of this hyperparameters in the analysis of this unit.

### Kullback-Leibler (KL) divergence and Its Properties

Recall the definition and properties of the Kullback-Leibler (KL) divergence between discrete distributions from the course Fundamentals of Statistics (Lecture 8).

Definition of KL-Divergence

Let  ğ  and  ğ  be discrete probability distributions wtih common sample space and with pmfs  ğ‘  and  ğ‘  respectively. Then the KL-divergence (also known as relative entropy ) of  ğ  from  ğ  is defined as

KL(ğ,ğ)=âˆ‘ğ‘¥âˆˆğ¸ğ‘(ğ‘¥)ln(ğ‘(ğ‘¥)ğ‘(ğ‘¥)). 
 
Properties of KL-Divergence

While the KL-divergence is not technically a distance between 2 distributions, it satisfies two useful properties of distances

KL(ğ,ğ)â‰¥0  (non-negative)

If  KL(ğ,ğ)=0 , then  ğ=ğ  (definiteness).

However, KL-divergence differs from a distance in the following ways:

KL(ğ,ğ)â‰ KL(ğ,ğ)  (assymmetry)

KL(ğ,ğ)â‰°KL(ğ,ğ«)+KL(ğ«,ğ)  (No triangle inequality).