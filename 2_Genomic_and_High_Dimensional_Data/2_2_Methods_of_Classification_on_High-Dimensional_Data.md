## Objectives

At the end of this lecture, you will be able to :

- Estimate classify data using Bayes Rule .

- Find the decision boundaries between two classes of data using quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA) .

- Use linear discriminant analysis to visualize data in lower dimensions.

- Perform logistic regression as a method of classification.

- Write down the optimization problem for support vector machines (SVM) .


## Classification Problem

Given data  ğ‘‹(1),ğ‘‹(2),â€¦,ğ‘‹(ğ‘›)  that take value in  â„ğ‘ . A classification problem is a problem in which we look for a function

ğ‘“:â„ğ‘â†’îˆ¯where îˆ¯={ğ¶1,â€¦ğ¶ğ‘˜} 
 
that maps the sample space of the data into a set of class labels  îˆ¯={ğ‘1,â€¦ğ‘ğ‘š} . This function is called a **classifer** .

The goal of a classification problem is to predict the class that an unseen data point belongs to. In this lecture, we will discuss classification methods in a **supervised learning setting**, where data with true class labels are available.

**Methods of Classification**

We will begin with methods with the most model assumptions and proceed to those with least assumptions:

- Classification using Bayes Rule

- Quadratic discriminant analysis (QDA) and Linear discriminant analysis (LDA)

- Logistic regression

- Support vector machine (SVM).

- Neural networks are also commonly used for classification. They are complex models defined by many parameters. Counter to intuition in classical statistics where overfitting generally occurs as models become too complex, neural networks generalize very well to unseen data, exactly due to the super over-parameterization of the model. We will not discuss neural networks.


### Bayes Rule
Given two events  ğ´  and  ğµ , Bayes' Rule says that

 	 ğ(ğ´|ğµ)=ğ(ğµ|ğ´)ğ(ğ´)ğ(ğµ). 	 	 
That is, the conditional probability of event  ğ´  given event  ğµ  can be written in terms of the conditional probability of event  ğµ  given event  ğ´  and the marginal probabilities  ğ(ğ´)  and  ğ(ğµ) .

For random variables  ğ‘‹ ,  Î˜ , Bayes' rule states analogously that

 	 ğ(Î˜=ğœƒ|ğ‘‹=ğ‘¥)=ğ(ğ‘‹=ğ‘¥|Î˜=ğœƒ)ğ(Î˜)ğ(ğ‘‹). 	 	 
In this lecture, we will generally be considering continuous variables  ğ‘‹  that take values in  â„ğ‘  modeling the data, and finite discrete variables  Î˜  respresenting the classes that we would like to categorize the data into.

### Bayes Rule for Classification

Let  ğ‘‹  be a random variable (or vector) modeling the data, and  ğ¶  be a random variable representating class labels. Assume the model  ğ(ğ‘‹|ğ¶)  for the data in different classes, and assume a prior distribution  ğ(ğ¶)  for the classes. We can then estimate the class label of data point  ğ‘¥  by finding the class that maximizes the posterior distribution  ğ(ğ¶|ğ‘‹=ğ‘¥) , which is given by Baye's rule:

 	 ğ(ğ¶|ğ‘‹)=ğ(ğ¶)ğ(ğ‘‹|ğ¶)ğ(ğ‘‹)âˆğ(ğ¶)ğ(ğ‘‹|ğ¶). 	 	 
(We dropped the overall factor  ğ(ğ‘‹)  since that does not affect which the maximization of  ğ(ğ¶|ğ‘‹) .)

The prior distribution  ğ(ğ¶)  of the class labels can be either based on prior studies and knowledge, or estimated from the prevalence in the training data.

In the special case of classification into only two classes, a decision boundary is a hypersurface such that data on one side are classified as one of the two classes, and data on the other side are classified into the other class.